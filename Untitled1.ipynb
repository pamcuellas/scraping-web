{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846265ac-5c48-4211-81f4-71a2cde3937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # <h1 style=\"color: purple;\">Step 1 - Scraping</h1>\n",
    "\n",
    "# ### NASA Mars News\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Import Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from random import *\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def init_browser():\n",
    "    #executable_path = {\"executable_path\": \"/app/.chromedriver/bin/chromedriver\"}\n",
    "    #return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "     executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "     return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "def scrape():\n",
    "    browser = init_browser()\n",
    "    json_data = {}\n",
    "\n",
    "    # Scrape the NASA Mars News Site here (https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest)\n",
    "    # and collect the LATEST News TITLE and PARAGRAPH Text. Assign the text to variables that you can reference later.\n",
    "    # Example:\n",
    "\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    print(\"#################################### STEP 1\")\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    random_num = randint(0, 39)\n",
    "    # Get the list of news \n",
    "    ul_list_news = soup.find(\"ul\", class_=\"item_list\")\n",
    "    # Get the random news\n",
    "    li_random_news = ul_list_news.find_all(\"li\")[random_num]\n",
    "    # Get the link for the random news.\n",
    "    link_latest_news = li_random_news.find(\"a\")[\"href\"]\n",
    "    # print(link_latest_news)\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "    # Go to the news page\n",
    "    url = url.replace(\"/news\",\"\",1) + link_latest_news\n",
    "    json_data['link_news'] = url\n",
    "\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "    # Get the title\n",
    "    last_title = soup.find(\"h1\", class_=\"article_title\")\n",
    "    last_title = last_title.text\n",
    "\n",
    "    # Get the paragraph list\n",
    "    lst_p = soup.find(\"div\", class_=\"wysiwyg_content\")\n",
    "    # Get the first paragraph\n",
    "    last_p = lst_p.find_all(\"p\")[0]\n",
    "    last_p = last_p.text\n",
    "\n",
    "    json_data[\"last_title\"] = last_title\n",
    "    json_data[\"last_p\"] = last_p\n",
    "    # ### JPL Mars Space Images - Featured Image\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "    # Go to the news page\n",
    "    #url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    url = \"https://www.jpl.nasa.gov/images?query=space\"\n",
    "    print(\"#################################### STEP 10\")\n",
    "\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # In[6]:\n",
    "\n",
    "    # Use splinter to navigate the site and find the image url for the current Featured \n",
    "    # Mars Image and assign the url string to a variable called featured_image_url.\n",
    "    # Get the Image URL\n",
    "\n",
    "    carousel_item = soup.find('SearchListingPageResults', class_=\"SearchResultCard-image\")['style']\n",
    "    featured_image_url = 'https://www.jpl.nasa.gov' + carousel_item.split(\"'\")[1]\n",
    "\n",
    "    #print(\"The URL is {}\".format(featured_image_url))\n",
    "\n",
    "    json_data[\"featured_image_url\"] = featured_image_url\n",
    "\n",
    "    # ### Mars Weather\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "    # Visit the Mars Weather twitter account here (https://twitter.com/marswxreport?lang=en) and scrape the latest \n",
    "    # Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather.\n",
    "    url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove tag a from tag p (Mars Weather).\n",
    "    for a in soup.find_all(\"a\", {'class':'twitter-timeline-link u-hidden'}): \n",
    "        a.decompose()\n",
    "\n",
    "    # Get the Mars Weather\n",
    "    ol = soup.find(\"ol\", class_=\"stream-items js-navigable-stream\")\n",
    "    li = ol.find_all(\"li\")[0]\n",
    "    div = li.find(\"div\", class_=\"js-tweet-text-container\")\n",
    "    mars_weather = div.find(\"p\").text + \".\"\n",
    "    json_data[\"mars_weather\"] = mars_weather\n",
    "    json_data[\"mars_weather_url\"] = url\n",
    "\n",
    "    # ### Mars Facts\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "    # Visit the Mars Facts webpage here (https://space-facts.com/mars/) and use Pandas to scrape \n",
    "    # the table containing facts about the planet including Diameter, Mass, etc.\n",
    "    url = \"https://space-facts.com/mars/\"\n",
    "    tables = pd.read_html(url)\n",
    "    df = tables[1]\n",
    "    df.rename(columns={0:\"Description\",1:\"Value\"}, inplace=True)\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "    # Use Pandas to convert the data to a HTML table string.\n",
    "    html_temp = df.to_html(index=False, escape=False, justify=\"left\", classes=\"table table-striped table-bordered table-sm\")\n",
    "\n",
    "    # Change the first column from td to th.\n",
    "    html_table = \"\"\n",
    "    for row in html_temp.split(\"<tr>\"): \n",
    "        row = row.replace(\"<td>\",\"<th>\",1)\n",
    "        row = row.replace(\"</td>\",\"</th>\",1)\n",
    "        html_table = html_table + \"<tr>\" + row\n",
    "    html_table = html_table.replace(\"<tr>\",\"\",1)\n",
    "\n",
    "    json_data[\"html_table\"] = html_table\n",
    "    json_data[\"mars_facts_url\"] = url\n",
    "    json_data[\"mars_hemispheres_url\"] = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "    # ### Mars Hemispheres\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "    # Visit the USGS Astrogeology site here (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars)\n",
    "    # to obtain high resolution images for each of Mar\"s hemispheres.\n",
    "    # url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    hemisphere_image_urls =[]\n",
    "    hemisphere_image_urls.append({\"title\":\"Cerberus Hemisphere\",        \"img_url\": \"https://astrogeology.usgs.gov/cache/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg\"}) \n",
    "    hemisphere_image_urls.append({\"title\":\"Schiaparelli Hemisphere\",    \"img_url\": \"https://astrogeology.usgs.gov/cache/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg\"}) \n",
    "    hemisphere_image_urls.append({\"title\":\"Syrtis Major Hemisphere\",    \"img_url\": \"https://astrogeology.usgs.gov/cache/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg\"}) \n",
    "    hemisphere_image_urls.append({\"title\":\"Valles Marineris Hemisphere\",\"img_url\": \"https://astrogeology.usgs.gov/cache/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg\"}) \n",
    "    \n",
    "    # browser.visit(url)\n",
    "    # html = browser.html\n",
    "    # soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # print(\"#################################### passo 4\")\n",
    "\n",
    "    # # Get text links to click\n",
    "    # items = soup.find_all(\"div\", class_=\"description\")\n",
    "    # lst_text =[]\n",
    "    # for item in items:\n",
    "    #     lst_text.append(item.h3.text)\n",
    "\n",
    "    # # Looping to click \n",
    "    # for i in range( len(lst_text) ):\n",
    "    #     browser.visit(url)\n",
    "    #     html = browser.html\n",
    "    #     soup = BeautifulSoup(html, \"html.parser\")\n",
    "    #     browser.click_link_by_partial_text(lst_text[i])\n",
    "\n",
    "    #     html = browser.html\n",
    "    #     soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "    #     # Get the image URL\n",
    "    #     tmp_img_url = soup.find('img',class_=\"wide-image\")[\"src\"]\n",
    "    #     img_url = \"https://astrogeology.usgs.gov\" + tmp_img_url\n",
    "\n",
    "    #     # Get the title\n",
    "    #     title = soup.title.text\n",
    "    #     title = title.split(\"|\")[0]\n",
    "    #     title = title.replace(\"Enhanced\",\"\")\n",
    "    #     title = title.strip()\n",
    "    #     # Append the dictionary with the image url string and the hemisphere title to a list. \n",
    "    #     hemisphere_image_urls.append({\"title\":title, \"img_url\": img_url})     \n",
    "    #     print(\"#################################### passo 5\")\n",
    "    #     break\n",
    "\n",
    "    # Add list of images URL and title to json_data \n",
    "    json_data[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "\n",
    "    # Close the browser window\n",
    "    browser.quit()\n",
    "\n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849c98b-9cff-4edf-96ce-e93679944592",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
